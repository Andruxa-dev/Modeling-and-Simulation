# Re-import required libraries after execution state reset
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
import pulp
from scipy.stats import skewnorm, skew  # Skewed normal distribution
########
import matplotlib.pyplot as plt
import seaborn as sns

##########################################################
#Generate synthetic daily work data
pd.set_option('display.float_format','{:.2f}'.format)
np.random.seed(42)  # Ensuring reproducibility

num_days = 500  # Simulate 200 days of work data


data = []


for day in range(1, num_days + 1):
    
    #GIven
    #task_types = ["Emails", "Meetings", "Coding", "Admin", "Break"]
    task_types = ["Coding","Break","Emails","Meetings","Admin"]
    
    ##########################################################
    #Uniform
    #There are always same task during the  day
    #selected_tasks = task_types 
    ##########################################################
    
    ##########################################################
    #Non Uniform
    num_tasks_today = np.random.randint(4,6) #Each day 4-6 tasks
    task_probs = [0.50,0.20,0.05,0.15,0.10]
    selected_tasks = np.random.choice(task_types, size=num_tasks_today, p=task_probs, replace = True) # With Replacement
    #selected_tasks = np.random.choice(task_types, size=5,replace = False) # Without replacement
    total_time_spent_per_day = 0
    
    for task in selected_tasks:
        
        #####################################################################################
        #Uniform distributions
#         time_spent = np.random.uniform(0.5, 3)  # Time spent (0.5 to 3 hours)
#         energy_level = np.random.uniform(0.5, 1.5)  # Energy multiplier (low=0.5, high=1.5)
#         interruptions = np.random.randint(0, 5)  # Number of interruptions
#         focus_level = np.random.uniform(0.2, 1.0)  # Focus ability (0.2 = low, 1.0 = high)
#         time_of_day = np.random.uniform(1, 8)  # Morning (1-4), Afternoon (5-8)
#         productivity = np.random.uniform(5, 25)       
        #####################################################################################
        
        
        
        #####################################################################################
        #Non Uniform distributions Introducing basic assumptions about data
        #---------------------
        #Some days have significantly longer work sessions
        # the more time spent on task the more the potential output
        #time_spent = skewnorm.rvs(a=6, loc=1.5, scale=1.2)  # Stronger right skew "a=6"
        
        #*********
        if total_time_spent_per_day >= 10:  # Stop adding tasks if total time reaches 10 hours
            break

        # Generate time spent per task (right-skewed)
        time_spent = min(10 - total_time_spent_per_day, skewnorm.rvs(a=6, loc=1.5, scale=1.2))  
        if time_spent < 0.5:  # Ensure a minimum of 0.5 hours per task
            time_spent = 0.5
        #*********
        #---------------------
        # Left skew for energy level (higher in morning, lower later, degrades over time)
        #Long gradual left tail--> slow and stead gradual decline over time
        #Steep right tail sudden energy drops due to intencity of tasks
        #Scale 0.3 assumes energy doesnt vary drastically every day, assumption that most people stay withing reasonable range
        #Scale 3 would give it a much higher spread, it woul dinvalidate asumption that most people start with relatively high energy
        #STD is a function of scale, skewness and Pi3.1416
        #Loc shifts distribution. best top be set between min max for meaningfull interpretation
        energy_level = skewnorm.rvs(a=-5, loc=1.2, scale=0.3)  # Left-skewed "a=-5"
        #---------------------
        # Left skew for focus level (focus decreases over time)
        base_focus_level = skewnorm.rvs(a=-4,loc=0.9,scale = 0.3)
        focus_level = base_focus_level
        #---------------------
        # Uniform random interruptions (not necessarily skewed)
        interruptions = np.random.randint(0, 5)
        #---------------------
        # Biased time of day choice (morning tasks more common)
        time_of_day = np.random.choice([2, 4, 6, 8], p=[0.4, 0.3, 0.2, 0.1]) # Morning (1-4, afternoon 5-8)
        #---------------------
        # Skew Productivity more while keeping randomness
        #Lets assume that there is more base productivity the more time is spent, so well add a right skew to the distribution
        base_productivity = skewnorm.rvs(a=8, loc=12, scale=5)
        # Ensure values stay within a reasonable range
        productivity = max(3, min(40, base_productivity))
        #####################################################################################
        
        
        #####################################################################################
        #Introduce dependency effect for hte independent varuiable
        #Assumptions
        #Focus level - Stronger focus level has the biggest impact on productivity as deep work maximizes output
        #Energy level - higher energy level leads to more focus and efficiency, contributing to higher productivity
        #time_spent - more time spent on a task generally increases productivity with deminishing returns
        #interruptions - more interruptions reduce efficiency by breaking concentration and slowing down work
        #time of day - later times of day reduce productivity due to fatigue, distractions etc
        
        dependency_effect =((focus_level * 0.40) + (energy_level * 0.35) + (time_spent * 0.2) - (interruptions * 0.17) - (time_of_day * 0.10) + np.random.normal(0.05)) # adding small noise
        #Will go with multiplication , scaled influence( not with weighted average or additive approach)
        productivity = base_productivity * (1 + dependency_effect)
        #productivity = base_productivity + dependency_effect
        productivity = max(3,min(40,productivity))
        #####################################################################################
        
        #####################################################################################
        #Introduce Focus - Task relationship weights
        #Assumptions
        # - Focus declines for repetitive and passive tasks
        # - Focus improves when doing engagaging, deep-wok tasks
        
#         focus_adjustment = {
#             "Coding":0.15, # Higher focus
#             "Break":-0.30, # Drops focus
#             "Emails":-0.10, #Slightly lower focus
#             "Meetings":-0.20, #PAssive engagement lower focus
#             "Admin":-0.25, #highly repetitive significant focus drop
#         }
        
#         adjusted_focus_level = base_focus_level + focus_adjustment.get(task,0)
#         adjusted_focus_level = max(0,min(1.5,adjusted_focus_level))
#         focus_level = adjusted_focus_level
        #####################################################################################
                               
        data.append([day, task, time_spent, energy_level, interruptions, focus_level, time_of_day, productivity])

df = pd.DataFrame(data, columns=["Day", "Task", "Time_Spent", "Energy_Level", "Interruptions", "Focus_Level", "Time_of_Day", "Productivity"])

########################
#Split the generated data set i nto train and test slices
percent_cut_30 = df.shape[0] * 0.30
#Select the 30% of the data set from the end
df_test_30percent = df.tail(int(percent_cut_30)).copy(deep = True)
########################
percent_cut_70 = df.shape[0] - percent_cut_30
df_train_70_percent = df.head(int(percent_cut_70)).copy(deep = True)
########################
#Reassign training data set to df
df = df_train_70_percent.copy(deep = True)

def plot_hist_with_stats(ax, data, title, color):
    # Plot histogram
    #bars = sns.histplot(data, kde=True, bins=25, color=color, ax=ax)
    bars = sns.histplot(data, kde=True, bins=15, color=color, ax=ax, edgecolor="black", linewidth=1.2)
    # Compute statistics
    mean_value = np.mean(data)
    median_value = np.median(data)
    
    # Calculate mode using histogram binning
    hist_values, bin_edges = np.histogram(data, bins=25)
    mode_value = bin_edges[np.argmax(hist_values)]  # Bin with highest frequency
    
    min_value = np.min(data)
    max_value = np.max(data)
    skewness = skew(data)
    std_dev = np.std(data)

    # Add vertical lines for mean, median, and mode
    ax.axvline(mean_value, color="black", linestyle="dashed", linewidth=2)
    ax.axvline(median_value, color="red", linestyle="solid", linewidth=2)
    ax.axvline(mode_value, color="green", linestyle="dotted", linewidth=2)

    # Add standard deviation shaded region
    ax.axvspan(mean_value - std_dev, mean_value + std_dev, alpha=0.2, color=color)

    # Insert text labels directly into the plot for mean, median, mode
    text_y = ax.get_ylim()[1] * 0.90  # Adjust vertical placement closer to top
    ax.text(mean_value, text_y, f"Mean: {mean_value:.2f}", fontsize=10, color="black", ha="center", bbox=dict(facecolor="white", edgecolor="black", boxstyle="round,pad=0.3"))
    ax.text(median_value, text_y * 0.60, f"Median: {median_value:.2f}", fontsize=10, color="red", ha="center", bbox=dict(facecolor="white", edgecolor="red", boxstyle="round,pad=0.3"))
    ax.text(mode_value, text_y * 0.40, f"Mode: {mode_value:.2f}", fontsize=10, color="green", ha="center", bbox=dict(facecolor="white", edgecolor="green", boxstyle="round,pad=0.3"))
    
    # Move Min/Max/Skew/STD inside the plot, at the top aligned neatly
    ax.annotate(
        f"Min: {min_value:.2f}  |  Max: {max_value:.2f}  |  Skew: {skewness:.2f}  |  STD: {std_dev:.2f}",
        xy=(0.5, 1.05), xycoords='axes fraction', fontsize=10, color="blue",
        bbox=dict(facecolor="white", edgecolor="blue", boxstyle="round,pad=0.3"),
        ha="center", va="top"
    )


    for patch in bars.patches:
        if isinstance(patch, plt.Rectangle):
            
            height = patch.get_height()
            if height >0:
                ax.text(patch.get_x() + patch.get_width() / 2,0,f"{int(height)}", fontsize=7, color="black",ha="center",va="bottom",fontweight="bold")

    # Adjust X and Y axis label sizes
    ax.set_xlabel(ax.get_xlabel(), fontsize=10)
    ax.set_ylabel(ax.get_ylabel(), fontsize=10)

    # Adjust X and Y axis tick label sizes
    ax.tick_params(axis='x', labelsize=9)
    ax.tick_params(axis='y', labelsize=9)


# Create figure with correct layout
fig, axes = plt.subplots(3, 3, figsize=(22, 18))


formula_latex = (
    r"$\mathbf{SciPy\ Skewness\ Formula:}$" "\n"
    r"$g_1 = \frac{\sum (x_i - \bar{x})^3 / N}{\left(\sum (x_i - \bar{x})^2 / N \right)^{3/2}}$"
)


# Add centered header **above** all plots
fig.suptitle(
    "Understanding Skewness in Distributions \n"
    "---------------------------------------------\n"
    " **Left-Skewed**: Mean < Median < Mode, longer tail on the left.\n"
    " **Symmetric**: Mean ≈ Median ≈ Mode, balanced distribution.\n"
    " **Right-Skewed**: Mean > Median > Mode, longer tail on the right.\n"
    "\n" 
    " **Why Does This Matter?**\n"
    " - Skewness affects statistical interpretations and predictive models.\n"
    " - In finance, right-skewness may indicate rare extreme profits,\n"
    "   while left-skewness can indicate frequent small losses.\n"
    " - For time management, focus and energy naturally decline over time,\n"
    "   leading to left-skewed distributions.\n"
    + formula_latex,
    fontsize=12, ha="center", y=1.04
)

# Create histograms with adjusted positioning for text inside the plot
plot_hist_with_stats(axes[0, 0], df["Productivity"], "Productivity Distribution", "blue")
plot_hist_with_stats(axes[0, 1], df["Time_Spent"], "Time Spent Distribution", "green")
plot_hist_with_stats(axes[0, 2], df["Energy_Level"], "Energy Level (Left-Skewed)", "red")
plot_hist_with_stats(axes[1, 0], df["Focus_Level"], "Focus Level (Left-Skewed)", "purple")
plot_hist_with_stats(axes[1, 1], df["Time_of_Day"], "Time of Day Distribution", "orange")
plot_hist_with_stats(axes[1, 2], df["Interruptions"], "Interruptions Distribution", "brown")
#####################################################################################
#Add bar chart for categorical data "task", just an additional plot
task_counts = df["Task"].value_counts(normalize=True) * 100
print("Task distributio in the dataset is:\n", task_counts)

sns.barplot(x=task_counts.index, y=task_counts.values, hue=task_counts.index, ax=axes[2,1], palette="viridis", legend=False)
axes[2,1].set_title("Task Distribution", fontsize=12)
axes[2,1].set_xlabel("Task", fontsize=10)
axes[2,1].set_ylabel("Count %", fontsize=10)
axes[2,1].tick_params(axis='x', rotation=45, labelsize=9)
axes[2,1].tick_params(axis='y',labelsize=9)

#Remove last subplot if not used
fig.delaxes(axes[2,2])
fig.delaxes(axes[2,0])
plt.tight_layout(rect=[0,0,1,0.98]) # adjust layout to fit suptitle
plt.show()

#####################################################################################
#Modify and create corr matrix view
df_encoded = pd.get_dummies(df,columns=["Task"], prefix="Task")
#Get corr matrix but excluding the Day column
corr_matrix = df_encoded.drop(columns=["Day"]).corr()

task_cols = [col for col in df_encoded.columns if "Task_" in col]
task_indices = [corr_matrix.columns.get_loc(col) for col in task_cols]

plt.figure(figsize=(10,6))
ax = sns.heatmap(corr_matrix, annot = True, cmap="coolwarm", fmt=".2f", linewidth=0.5, annot_kws={"size":8})

if task_indices:
    min_idx, max_idx =min(task_indices), max(task_indices)
    rect = plt.Rectangle((min_idx,min_idx), len(task_cols), len(task_cols), linewidth=5, edgecolor='black', facecolor ='none')
    ax.add_patch(rect)
    
plt.title("Correlation MAtrix of WorkDay Metrix",fontsize = 12)
plt.show()





#####################################################################################
#Encode categprical variable
df_encoded = pd.get_dummies(df,columns=["Task"], prefix="Task")

X = df_encoded.drop(columns=["Day","Productivity"])
y = df_encoded["Productivity"]

model = LinearRegression()
model.fit(X,y)

#Display coefficients
coef_df = pd.DataFrame({"Feature": X.columns, "Coefficient":model.coef_})
#####################################################################################
intercept = model.intercept_
regression_formula =f"Productivity = {intercept:.10f}"

for feature, coef in zip(X.columns, model.coef_):
    
    sign = "+" if coef >=0 else "-"
    regression_formula +=f" {sign} {abs(coef):.3f} * {feature}"
    
#Dispay the regression formula
#####################################################################################
#ok lets construct regression summary
n = X.shape[0] # number of observations
p = X.shape[1] # number of predictors

intercept = model.intercept_
r_squared = model.score(X,y)
adj_r_squared = 1 - (1 - r_squared) * (n-1) / (n-p-1)

print("----------------")
print("DEPENDENT VARIABLE IS PRODUCTIVITY")
print("INDEPENDENT VARIABLES ARE: ['Time_Spent','Energy_Level','Interruptions','Focus_level','Time_of_Day','Task_Admin','Task_Break','Task_Coding','Task_Emails','Task_Meetings']")
print(f"MULTIPLE LINEAR REGRESSION EQUATION IS: {regression_formula}")
print(f"Summary, number of observation is: {n}, number of predictors is: {p}, intercept is: {intercept}, r_squared is: {r_squared}, adjusted r_squared is: {adj_r_squared}")
print("----------------")

#####################################################################################
#Plot model feature importance
y_pred = model.predict(X)
residuals = y - y_pred

coef_df_sorted = coef_df.sort_values(by="Coefficient", ascending = False)
coef_df_sorted.plot(kind="barh",x="Feature",y="Coefficient", legend = False)
plt.xlabel("Coefficient Value")
plt.ylabel("Feature")
plt.title("Feature importance")
plt.grid()

plt.show()
#####################################################################################
import statsmodels.api as sm

# Compute LOWESS (Locally Weighted Scatterplot Smoothing)
lowess = sm.nonparametric.lowess(residuals, y_pred, frac=0.3)

plt.figure(figsize=(10, 5))
sns.scatterplot(x=y_pred, y=residuals, alpha=0.5)
plt.plot(lowess[:, 0], lowess[:, 1], color="red", linewidth=2, label="LOWESS Fit")
plt.axhline(0, color="black", linestyle="dashed", linewidth=1)
plt.xlabel("Predicted Productivity")
plt.ylabel("Residuals (Actual - Predicted)")
plt.title("Residual Plot: Checking Model Fit")
plt.legend()
plt.show()


###############################################################################
import statsmodels.api as sm

X = X.astype("float")

X_with_intercept = sm.add_constant(X)  # Add intercept column
model_sm = sm.OLS(y, X_with_intercept).fit()

print(model_sm.summary())  # Shows SE, p-values, and confidence intervals
model_se = np.sqrt(model_sm.mse_resid)
print(f"Standard Error of the Model (±SE): {model_se:.4f}")
